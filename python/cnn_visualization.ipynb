{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains several approaches for representing in a graphical way the journey of a brain image while going to the set neural network architecture. The advantage of a graphical visualization is being able to recognize which features (i.e: edges, pattern, etc) have been captured in each convolutional and pooling layer by the CNN-Model. This l\n",
    "\n",
    "For this notebook following Data Set is used, containing only 2D images of MRI Segmentation: [Alzheimer's Dataset (4 class of Images)](https://www.kaggle.com/datasets/tourist55/alzheimers-dataset-4-class-of-images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import metrics\n",
    "import keras.backend as K\n",
    "import string\n",
    "from IPython.display import Image, display\n",
    "#from CAM import get_heatmap\n",
    "#!pip install keras-visualizer\n",
    "from keras_visualizer import visualizer\n",
    "#!pip install graphviz\n",
    "\n",
    "#!pip install imutils\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, load_img, img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pfads for both training and validation data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with training pictures\n",
    "train_dir = os.path.join('/Users/josealbertodiazsalas/Downloads/Alzheimers_Dataset/train')\n",
    "\n",
    "# Directory with validation pictures\n",
    "test_dir = os.path.join('/Users/josealbertodiazsalas/Downloads/Alzheimers_Dataset/validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define ImageDataGenerator for both train and validation data. Image augmentation can be also set in this step\n",
    "# All images will be rescaled by 1./255 - Normalization\n",
    "# Open point: how to deal with unbalanced data\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                  dtype=tf.float32,\n",
    "                                  validation_split = 0.2,           # setting a validation data set from training data set  \n",
    "                                  #rotation_range = 90,\n",
    "                                  #width_shift_range = 0.2,\n",
    "                                  #height_shift_range = 0.2,\n",
    "                                  #shear_range = 0.2,\n",
    "                                  #zoom_range = 0.2,\n",
    "                                  horizontal_flip = True,\n",
    "                                  #vertical_flip = True,\n",
    "                                  #fill_mode='nearest',\n",
    "                                  #data_format = 'channels_last'\n",
    "                                  )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_generator = datagen.flow_from_directory(train_dir,                        # This is the source directory for training images, previously defined\n",
    "                                                   batch_size = 64,\n",
    "                                                   target_size = (150, 150),   # All images will be resized to 150x150\n",
    "                                                   color_mode = 'grayscale',   # RGB, if working with colors\n",
    "                                                   class_mode = 'sparse',      # for integer labels representation, not hot encoding\n",
    "                                                   shuffle = True,             # to get the data randomized\n",
    "                                                   subset = 'training',\n",
    "                                                   seed = 123                  # if we want to achieved the same results  \n",
    "                                                   )          \n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = datagen.flow_from_directory(train_dir,\n",
    "                                                   batch_size  = 64,\n",
    "                                                   target_size = (150, 150),\n",
    "                                                   color_mode = 'grayscale',\n",
    "                                                   class_mode  = 'sparse',\n",
    "                                                   subset = 'validation',\n",
    "                                                   shuffle = True,\n",
    "                                                   seed = 123\n",
    "                                                   )\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "                                                   batch_size  = 64,\n",
    "                                                   target_size = (150, 150),\n",
    "                                                   color_mode = 'grayscale',\n",
    "                                                   class_mode  = 'sparse',        \n",
    "                                                   )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training Data contains {train_generator.samples} images from type {train_generator.dtype}, following Labels are assigned to the classes: {train_generator.class_indices}')\n",
    "print('-'*30)\n",
    "print(f'Validation Data contains {validation_generator.samples} images from type {validation_generator.dtype}, following Labels are assigned to the classes: {validation_generator.class_indices}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function will plot images in the form of a grid with 1 row x 10 columns\n",
    "train_images, train_labels = next(train_generator)\n",
    "validation_images, validation_labels = next(validation_generator)\n",
    "test_images, test_labels = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "   fig, axes = plt.subplots(1,20,figsize=(20,20))\n",
    "   axes = axes.flatten()\n",
    "   for img, ax in zip(images_arr, axes):\n",
    "       #print(img.shape)\n",
    "       ax.imshow(img)\n",
    "       ax.axis('off')\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "plotImages(train_images)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageDataGenerator fit() accepts x4 dimensions (number of, width, height, channel)\n",
    "\n",
    "x=np.concatenate([train_generator.next()[0] for i in range(train_generator.__len__())])\n",
    "y=np.concatenate([validation_generator.next()[0] for i in range(validation_generator.__len__())])\n",
    "a=np.concatenate([train_generator.next()[1] for i in range(train_generator.__len__())])\n",
    "b=np.concatenate([validation_generator.next()[1] for i in range(validation_generator.__len__())])\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_model():\n",
    "\n",
    "   model = tf.keras.models.Sequential([\n",
    "   # Add convolutions and max pooling\n",
    "   tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 1)),\n",
    "   tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "   tf.keras.layers.MaxPooling2D(2, 2),\n",
    "\n",
    "   tf.keras.layers.Conv2D(64, (3,3), activation='relu', name='LastConvLayer'),\n",
    "   tf.keras.layers.MaxPooling2D(2,2),\n",
    "\n",
    "   # Add dense layers\n",
    "   tf.keras.layers.Flatten(),\n",
    "   tf.keras.layers.Dense(64, activation='relu'),\n",
    "   tf.keras.layers.Dense(4, activation='softmax'),\n",
    "   ])\n",
    "\n",
    "   # Compile the model for multiclass problem\n",
    "   model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['acc']\n",
    "                 )\n",
    "      \n",
    "   return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your untrained model\n",
    "model = convolutional_model()\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Model Architecture\n",
    "visualizer(model, filename='moderateDem50',format='jpg', view=False)\n",
    "from PIL import Image\n",
    "input_path = './moderateDem50.jpg'\n",
    "im = Image.open(input_path)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model\n",
    "print(f'\\nMODEL TRAINING:')\n",
    "history = model.fit(train_generator,      #we might need to defined a Callback as convergence (stopping) criteria\n",
    "                   epochs=15,\n",
    "                   verbose = 2,\n",
    "                   validation_data = validation_generator,\n",
    "                   #steps_per_epoch = 8      # numbers of images / batch_size?\n",
    "                   #validation_steps = len(validation_set)/batch_size\n",
    "                   )   \n",
    "\n",
    "# Evaluate on the test set\n",
    "#print(f'\\nMODEL EVALUATION:')\n",
    "#test_loss = model.evaluate_generator(generator=validation_generator)\n",
    "\n",
    "\n",
    "print(f\"Your model was trained for {len(history.epoch)} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1_Visualizing the convolutions and pooling using Keras.layers API\n",
    "\n",
    "By picking out a few instances of the AD classes it is possible to visualize the journey of an brain image through the convolutions. In this code we also can set the CONVOLUTION_NUMBER variable to look at the output of a specific convolution and thus identify which patterns are being detected.\n",
    "\n",
    "The Keras API gives us each convolution and each pooling and each dense, etc. as a individual layer. So with the layers API, we can take a look at each layer's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's print out the first X validation labels\n",
    "print(f'Validation Data set contains following Labels: {validation_generator.class_indices}')\n",
    "print(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for x3 images, representing x4 conv layer from left to right\n",
    "f, axarr = plt.subplots(3,4)\n",
    "plt.rcParams[\"figure.figsize\"] =(20,9)\n",
    "\n",
    "# Following images refer to its index within the label array shown above\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=1\n",
    "THIRD_IMAGE=2\n",
    "CONVOLUTION_NUMBER = 24  \n",
    "\n",
    "# Create a list of each layer's output\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "\n",
    "# We can treat each item in the layer as an individual activation model\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "\n",
    "# By looping through the layers, we can display the journey of the image through the first convolution, first pooling, second convolution, and so on\n",
    "for x in range(0,4):\n",
    " f1 = activation_model.predict(validation_images[FIRST_IMAGE].reshape(1, 150, 150, 1))[x]\n",
    " axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    " axarr[0,x].grid(False)\n",
    "  f2 = activation_model.predict(validation_images[SECOND_IMAGE].reshape(1, 150, 150, 1))[x]\n",
    " axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='viridis')\n",
    " axarr[1,x].grid(False)\n",
    "  f3 = activation_model.predict(validation_images[THIRD_IMAGE].reshape(1, 150, 150, 1))[x]\n",
    " axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='plasma')\n",
    " axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2_Visualization intermediate representations (hardcoded)\n",
    "\n",
    "This part includes visualizations of the brain image as it passes through the convolutions and pooling layers. We generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. \n",
    "\n",
    "In following code, we randomly select a single picture from the training set corresponding to a specific class. By running the code again, we can generate intermediate representations for a variety of training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with training pictures\n",
    "ModerateDemented_dir = os.path.join('/Users/josealbertodiazsalas/Downloads/Alzheimers_Dataset/train/ModerateDemented')\n",
    "ModerateDemented_names = os.listdir(ModerateDemented_dir)\n",
    "\n",
    "# Random input image from the training set\n",
    "ModerateDemented_file = [os.path.join(ModerateDemented_dir, f) for f in ModerateDemented_names]\n",
    "img_path = random.choice(ModerateDemented_file)\n",
    "\n",
    "img = load_img(img_path, target_size=(150, 150), color_mode = 'grayscale')\n",
    "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
    "\n",
    "# Scale by 1/255\n",
    "x /= 255\n",
    "\n",
    "# Run the image through the network, thus obtaining all intermediate representations for this image.\n",
    "successive_feature_maps = visualization_model.predict(x)\n",
    "\n",
    "\n",
    "# These are the names of the layers, so you can have them as part of the plot\n",
    "layer_names = [layer.name for layer in model.layers[1:]]\n",
    "\n",
    "# Display the representations\n",
    "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
    " if len(feature_map.shape) == 4:\n",
    "\n",
    "   # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
    "   n_features = feature_map.shape[-1]  # number of features in feature map\n",
    "\n",
    "   # The feature map has shape (1, size, size, n_features)\n",
    "   size = feature_map.shape[1]\n",
    "  \n",
    "   # Tile the images in this matrix\n",
    "   display_grid = np.zeros((size, size * n_features))\n",
    "   for i in range(n_features):\n",
    "     x = feature_map[0, :, :, i]\n",
    "     x -= x.mean()\n",
    "     x /= x.std()\n",
    "     x *= 64\n",
    "     x += 128\n",
    "     x = np.clip(x, 0, 255).astype('uint8')\n",
    "  \n",
    "     # Tile each filter into this big horizontal grid\n",
    "     display_grid[:, i * size : (i + 1) * size] = x\n",
    "  \n",
    "   # Display the grid\n",
    "   scale = 20. / n_features\n",
    "   plt.figure(figsize=(scale * n_features, scale))\n",
    "   plt.title(layer_name)\n",
    "   plt.grid(False)\n",
    "   plt.imshow(display_grid, aspect='auto', cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3_Keras Grad-CAM class activation visualization\n",
    "\n",
    "Grad-CAM uses the gradients of any target concept (i.e. classes), flowing into the last convolutional layer of the CNN architecture to produce a coarse localization map highlighting the important regions in the image - captured features - for predicting the concept. The output of Grad-CAM is a heatmap visualization for a given class label. \n",
    "\n",
    "Using Grad-CAM and its resulting heatmap, we can visually validate where our network is looking, verifying that it is indeed looking at the correct patterns in the image and activating around those patterns. \n",
    "\n",
    "More information to be found on https://keras.io/examples/vision/grad_cam/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with training pictures\n",
    "ModerateDemented_dir = os.path.join('/Users/josealbertodiazsalas/Downloads/Alzheimers_Dataset/train/ModerateDemented')\n",
    "ModerateDemented_names = os.listdir(ModerateDemented_dir)\n",
    "\n",
    "# Random input image from the training set\n",
    "ModerateDemented_file = [os.path.join(ModerateDemented_dir, f) for f in ModerateDemented_names]\n",
    "img_path = random.choice(ModerateDemented_file)\n",
    "\n",
    "img = load_img(img_path, target_size=(150, 150), color_mode = 'grayscale')\n",
    "img_array = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
    "img_array = img_array.reshape((1,) + img_array.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
    "#x = preprocess_input(x)\n",
    "\n",
    "'''\n",
    "def get_img_array(img_path, size):\n",
    "   img = keras.preprocessing.image.load_img(img_path, target_size=(size))\n",
    "   array = keras.preprocessing.image.img_to_array(img)\n",
    "   # We add a dimension to transform our array into a \"batch\" of size (1, 150, 150, 1)\n",
    "   array = np.expand_dims(array, axis=0)\n",
    "   return array\n",
    "'''\n",
    "\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "   # First, we create a model that maps the input image to the activations of the last conv layer as well as the output predictions\n",
    "   grad_model = tf.keras.models.Model(\n",
    "       [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
    "   )\n",
    "\n",
    "   # Then, we compute the gradient of the top predicted class for our input image\n",
    "   # with respect to the activations of the last conv layer\n",
    "   with tf.GradientTape() as tape:\n",
    "       last_conv_layer_output, preds = grad_model(img_array)\n",
    "       if pred_index is None:\n",
    "           pred_index = tf.argmax(preds[0])\n",
    "       class_channel = preds[:, pred_index]\n",
    "\n",
    "   # This is the gradient of the output neuron (top predicted or chosen) with regard to the output feature map of the last conv layer\n",
    "   grads = tape.gradient(class_channel, last_conv_layer_output)\n",
    "\n",
    "   # This is a vector where each entry is the mean intensity of the gradient over a specific feature map channel\n",
    "   pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "   # We multiply each channel in the feature map array by \"how important this channel is\" with regard to the top predicted class\n",
    "   # then sum all the channels to obtain the heatmap class activation\n",
    "   last_conv_layer_output = last_conv_layer_output[0]\n",
    "   heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
    "   heatmap = tf.squeeze(heatmap)\n",
    "\n",
    "   # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "   heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
    "   plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "   return heatmap.numpy()\n",
    "\n",
    "last_conv_layer_name = \"LastConvLayer\"\n",
    "\n",
    "# Remove last layer's softmax\n",
    "#model.layers[-1].activation = None\n",
    "\n",
    "# Print what the top predicted class is\n",
    "preds = model.predict(img_array)\n",
    "\n",
    "# Generate class activation heatmap\n",
    "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
    "\n",
    "# Display heatmap\n",
    "plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "plt.matshow(heatmap)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_and_display_gradcam(img_path, heatmap, cam_path=\"cam.jpg\", alpha=2):\n",
    "   # Load the original image\n",
    "   img = keras.preprocessing.image.load_img(img_path)\n",
    "   img = keras.preprocessing.image.img_to_array(img)\n",
    "\n",
    "   # Rescale heatmap to a range 0-255\n",
    "   heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "   # Use jet colormap to colorize heatmap\n",
    "   jet = plt.cm.get_cmap(\"jet\")\n",
    "\n",
    "   # Use RGB values of the colormap\n",
    "   jet_colors = jet(np.arange(256))[:, :3]\n",
    "   jet_heatmap = jet_colors[heatmap]\n",
    "\n",
    "   # Create an image with RGB colorized heatmap\n",
    "   jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "   jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "   jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "\n",
    "   # Superimpose the heatmap on original image\n",
    "   superimposed_img = jet_heatmap * alpha + img\n",
    "   superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "\n",
    "   # Save the superimposed image\n",
    "   #superimposed_img.save(cam_path)\n",
    "\n",
    "   # Display Grad CAM\n",
    "   plt.rcParams[\"figure.figsize\"] = (18,8)\n",
    "   display(Image(cam_path))\n",
    "\n",
    "\n",
    "display(Image(img_path))\n",
    "plt.matshow(heatmap)\n",
    "save_and_display_gradcam(img_path, heatmap)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4bbe81aeeb531c126008c724994955906b0f990de7c9493984586aab713c420"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
